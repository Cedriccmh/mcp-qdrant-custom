# ========================================
# MCP Server Qdrant Configuration
# ========================================
# Copy this file to .env and fill in your actual values

# ========================================
# Qdrant Connection Settings
# ========================================

# Qdrant server URL (use either QDRANT_URL or QDRANT_LOCAL_PATH, not both)
QDRANT_URL=http://localhost:6333

# Qdrant API key (optional, for cloud instances)
# QDRANT_API_KEY=your-api-key-here

# Local Qdrant storage path (use instead of QDRANT_URL for local mode)
# QDRANT_LOCAL_PATH=./qdrant_data

# Docker Qdrant data volume path (for Docker deployments)
# QDRANT_DATA_PATH=./qdrant_data

# ========================================
# Collection Settings
# ========================================

# Default collection name
COLLECTION_NAME=your-collection-name

# Maximum number of search results to return
QDRANT_SEARCH_LIMIT=20

# Read-only mode (true/false) - prevents write operations
QDRANT_READ_ONLY=false

# Minimum similarity score threshold for search results (0.0-1.0 for cosine similarity)
# Results with scores below this threshold will be filtered out
# Default is None (no filtering). Recommended range: 0.3-0.7
# QDRANT_SCORE_THRESHOLD=0.5

# Allow arbitrary filter queries (true/false)
QDRANT_ALLOW_ARBITRARY_FILTER=false

# ========================================
# Embedding Provider Settings
# ========================================

# Embedding provider type: fastembed, openai_compatible
EMBEDDING_PROVIDER=openai_compatible

# Embedding model name
# For fastembed: sentence-transformers/all-MiniLM-L6-v2
# For OpenAI: text-embedding-ada-002, text-embedding-3-small, text-embedding-3-large
# For Ollama: nomic-embed-text, mxbai-embed-large
# For other providers: Qwen/Qwen3-Embedding-8B, etc.
EMBEDDING_MODEL=Qwen/Qwen3-Embedding-8B

# ========================================
# OpenAI Compatible Settings
# ========================================

# API key for OpenAI or compatible services
# For OpenAI: sk-...
# For Ollama (local): can be omitted or set to any value
# For other providers: provider-specific key
OPENAI_API_KEY=sk-your-api-key-here

# Base URL for OpenAI API or compatible services
# For OpenAI: https://api.openai.com/v1
# For Ollama: http://localhost:11434/v1
# For Azure OpenAI: https://your-resource.openai.azure.com/openai/deployments/your-deployment
# For SiliconFlow: https://api.siliconflow.cn/v1
OPENAI_BASE_URL=https://api.siliconflow.cn/v1

# Vector size for the embedding model
# Common sizes:
# - OpenAI text-embedding-ada-002: 1536
# - OpenAI text-embedding-3-small: 1536
# - OpenAI text-embedding-3-large: 3072
# - Qwen/Qwen3-Embedding-8B: 4096
# - nomic-embed-text (Ollama): 768
OPENAI_VECTOR_SIZE=4096

# Request timeout in seconds (optional, default: 30.0)
OPENAI_TIMEOUT=30.0

# ========================================
# Server Settings
# ========================================

# HTTP/SSE server port (for non-stdio transports)
PORT=8765

# FastMCP port (usually same as PORT)
FASTMCP_PORT=8765

# Python unbuffered output (recommended: 1)
PYTHONUNBUFFERED=1

# ========================================
# Logging Settings
# ========================================

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=DEBUG

# ========================================
# Tool Descriptions (Optional)
# ========================================

# Custom description for the qdrant-store tool
# TOOL_STORE_DESCRIPTION=Keep the memory for later use, when you are asked to remember something.

# Custom description for the qdrant-find tool
# TOOL_FIND_DESCRIPTION=Look up memories in Qdrant. Use this tool when you need to: \n - Find memories by their content \n - Access memories for further analysis \n - Get some personal information about the user

